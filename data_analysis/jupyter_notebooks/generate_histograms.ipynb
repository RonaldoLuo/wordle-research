{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as api\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel\n",
    "from framework import WordleAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: for multilingual WordNet data\n",
    "nltk.download('punkt')     # Optional: for tokenization\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Function to get the synsets of the word\n",
    "def get_synsets(word):\n",
    "    return wn.synsets(word)\n",
    "\n",
    "def calc_path_similarity(word1, word2):\n",
    "    synsets1 = get_synsets(word1)\n",
    "    synsets2 = get_synsets(word2)\n",
    "    \n",
    "    # Ensure words have synsets (not all words exist in WordNet)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return -1000\n",
    "    \n",
    "    # Initialize maximum similarity\n",
    "    max_sim = -1000\n",
    "    \n",
    "    # Iterate through all synset pairs and calculate the similarity\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            sim = synset1.path_similarity(synset2)\n",
    "            \n",
    "            # Update maximum similarity found\n",
    "            if sim is not None and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    \n",
    "    return max_sim\n",
    "\n",
    "def calc_wup_similarity(word1, word2):\n",
    "    synsets1 = get_synsets(word1)\n",
    "    synsets2 = get_synsets(word2)\n",
    "    \n",
    "    # Ensure words have synsets (not all words exist in WordNet)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return -1000\n",
    "    \n",
    "    # Initialize maximum similarity\n",
    "    max_sim = -1000\n",
    "    \n",
    "    # Iterate through all synset pairs and calculate the similarity\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            sim = synset1.wup_similarity(synset2)\n",
    "            \n",
    "            # Update maximum similarity found\n",
    "            if sim is not None and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    \n",
    "    return max_sim\n",
    "\n",
    "def calc_lch_similarity(word1, word2):\n",
    "    synsets1 = get_synsets(word1)\n",
    "    synsets2 = get_synsets(word2)\n",
    "    \n",
    "    # Ensure words have synsets (not all words exist in WordNet)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return -1000\n",
    "    \n",
    "    # Initialize maximum similarity\n",
    "    max_sim = -1000\n",
    "    \n",
    "    # Iterate through all synset pairs and calculate the similarity\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            try:\n",
    "                sim = synset1.lch_similarity(synset2)\n",
    "            except:\n",
    "                sim = None\n",
    "            \n",
    "            # Update maximum similarity found\n",
    "            if sim is not None and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    \n",
    "    return max_sim\n",
    "\n",
    "def get_pairwise_similarity(data, sim_function):\n",
    "    human_similarity, optimal_similarity = {}, {}\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        state = row['state']\n",
    "        previous_guesses = eval(row['previous_guesses'])\n",
    "        optimal_guesses = eval(row['optimal_guess'])\n",
    "        human_guesses = eval(row['human_guess'])\n",
    "\n",
    "        human_similarity[state] = []\n",
    "        optimal_similarity[state] = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in range(len(previous_guesses)):\n",
    "            prev = previous_guesses[i][-1].lower()\n",
    "            opt = optimal_guesses[i].lower()\n",
    "            hum = human_guesses[i].lower()\n",
    "            try:\n",
    "                opt_sim = sim_function(prev, opt)\n",
    "                hum_sim = sim_function(prev, hum)\n",
    "                if opt_sim == -1000 or hum_sim == -1000:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            except:\n",
    "                # print(glove_distance(prev, opt, model))\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "\n",
    "            optimal_similarity[state].append(opt_sim)\n",
    "            human_similarity[state].append(hum_sim)\n",
    "\n",
    "        # print(f\"Skipped {skipped} words for {state}\")\n",
    "\n",
    "\n",
    "    return human_similarity, optimal_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe model (300 dimensions, trained on Wikipedia + Gigaword)\n",
    "glove_model = api.load('glove-wiki-gigaword-300')\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "\n",
    "# Function to compute distance between two words using GloVe\n",
    "def glove_distance(word1, word2, model):\n",
    "    if word1 in model and word2 in model:\n",
    "        vec1 = model[word1]\n",
    "        vec2 = model[word2]\n",
    "        \n",
    "        # Calculate Cosine Similarity\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        distance = 1 - similarity  # Cosine distance\n",
    "        \n",
    "        return similarity, distance\n",
    "    else:\n",
    "        return f\"One or both words ('{word1}', '{word2}') not found in the GloVe model.\"\n",
    "\n",
    "def word2vec_distance(word1, word2, model):\n",
    "    if word1 in model and word2 in model:\n",
    "        vec1 = model[word1]\n",
    "        vec2 = model[word2]\n",
    "        \n",
    "        # Calculate Cosine Similarity\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        distance = 1 - similarity  # Cosine distance\n",
    "        \n",
    "        return similarity, distance\n",
    "    else:\n",
    "        return f\"One or both words ('{word1}', '{word2}') not found in the Word2Vec model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax = pd.read_csv('data/state_data_minimax.csv')\n",
    "entropy = pd.read_csv('data/state_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_distance(data, sim_function, model):\n",
    "    human_distance, optimal_distance = {}, {}\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        state = row['state']\n",
    "        previous_guesses = eval(row['previous_guesses'])\n",
    "        optimal_guesses = eval(row['optimal_guess'])\n",
    "        human_guesses = eval(row['human_guess'])\n",
    "\n",
    "        human_distance[state] = []\n",
    "        optimal_distance[state] = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in range(len(previous_guesses)):\n",
    "            prev = previous_guesses[i][-1].lower()\n",
    "            opt = optimal_guesses[i].lower()\n",
    "            hum = human_guesses[i].lower()\n",
    "            try:\n",
    "                opt_sim, opt_dist = sim_function(prev, opt, model)\n",
    "                hum_sim, hum_dist = sim_function(prev, hum, model)\n",
    "            except:\n",
    "                # print(glove_distance(prev, opt, model))\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "\n",
    "            optimal_distance[state].append(opt_dist)\n",
    "            human_distance[state].append(hum_dist)\n",
    "\n",
    "        # print(f\"Skipped {skipped} words for {state}\")\n",
    "\n",
    "\n",
    "    return human_distance, optimal_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_with_previous(data, sim_function):\n",
    "    human_similarity, optimal_similarity = {}, {}\n",
    "\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        state = row['state']\n",
    "        previous_guesses = eval(row['previous_guesses'])\n",
    "        optimal_guesses = eval(row['optimal_guess'])\n",
    "        human_guesses = eval(row['human_guess'])\n",
    "\n",
    "        human_similarity[state] = []\n",
    "        optimal_similarity[state] = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in range(len(previous_guesses)):\n",
    "            prev = previous_guesses[i].lower()\n",
    "            opt = optimal_guesses[i].lower()\n",
    "            hum = human_guesses[i].lower()\n",
    "            hum_avg = 0\n",
    "            opt_avg = 0\n",
    "            for p in prev:\n",
    "                hum_avg += sim_function(p, hum)\n",
    "                opt_avg += sim_function(p, opt)\n",
    "            opt_avg /= len(prev)\n",
    "            hum_avg /= len(prev)\n",
    "                    \n",
    "            \n",
    "\n",
    "            optimal_similarity[state].append(opt_avg)\n",
    "            human_similarity[state].append(hum_avg)\n",
    "\n",
    "        # print(f\"Skipped {skipped} words for {state}\")\n",
    "\n",
    "\n",
    "    return human_similarity, optimal_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_distance('dad', 'mom', w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(analyze.levenshtein_between_guesses('apple', 'tester'))\n",
    "# print(analyze.common_syllables('apple', 'tester'))\n",
    "# print(analyze.shared_chars('apple', 'tester'))\n",
    "# WordleAnalyzer.levenshtein_between_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "human_similarity, optimal_similarity = get_pairwise_similarity(entropy, WordleAnalyzer.shared_chars)\n",
    "file_path = 'new_shared_chars/entropy/shared_chars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((human_similarity[14])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cohen's d\n",
    "def cohen_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_state(state):\n",
    "    written_state = [0] * 3\n",
    "    written_state[0] = state//100\n",
    "    written_state[1] = (state%100)//10\n",
    "    written_state[2] = state%10\n",
    "    return f'{written_state[0]}g{written_state[1]}y{written_state[2]}b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((human_similarity[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(human_distance, optimal_distance, title):\n",
    "    # make a new pandas dataframe, with the column names state, cohen distance, t statistic, p value, number of samples\n",
    "    stats_data = pd.DataFrame(columns=['state', 'Cohen Distance', 't_statistic', 'p_value', 'num_samples', 'human_mean', 'human_std', 'optimal_mean', 'optimal_std'])\n",
    "\n",
    "\n",
    "    for state in human_distance.keys():\n",
    "        cohen_dist = cohen_d(human_distance[state], optimal_distance[state])\n",
    "        statistic, p_value = ttest_rel(human_distance[state], optimal_distance[state])\n",
    "        number = len(human_distance[state])\n",
    "        human_mean = np.mean(human_distance[state])\n",
    "        human_std = np.std(human_distance[state])\n",
    "        optimal_mean = np.mean(optimal_distance[state])\n",
    "        optimal_std = np.std(optimal_distance[state])\n",
    "        stats_data = stats_data.append({'state': prettify_state(state), 'Cohen Distance': cohen_dist, 't_statistic': statistic, 'p_value': p_value, 'num_samples': number, 'human_mean': human_mean, 'human_std': human_std, 'optimal_mean': optimal_mean, 'optimal_std': optimal_std}, ignore_index=True)\n",
    "\n",
    "    stats_data.to_csv(f'{title}_statistics.csv', index=False)\n",
    "    plt.figure()\n",
    "    sns.histplot(stats_data['Cohen Distance'])\n",
    "\n",
    "    plt.savefig(f'{title}_cohen_distance.pdf', format='pdf')\n",
    "    return stats_data\n",
    "\n",
    "\n",
    "    # make a new pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_statistics(human_similarity, optimal_similarity, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(human_similarity[14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 1\n",
    "max = 0\n",
    "for state, value in human_similarity.items():\n",
    "    for v in value:\n",
    "        if v < min:\n",
    "            min = v\n",
    "        if v > max:\n",
    "            max = v\n",
    "\n",
    "for state, value in optimal_similarity.items():\n",
    "    for v in value:\n",
    "        if v < min:\n",
    "            min = v\n",
    "        if v > max:\n",
    "            max = v\n",
    "\n",
    "print(min, max)\n",
    "print(human_similarity.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_histograms(state, human_distance, optimal_distance, title, x_axis, type, discrete=False):\n",
    "    plt.figure()\n",
    "    sns.set_palette(\"bright\")\n",
    "    data = pd.DataFrame({x_axis: human_distance[state] + optimal_distance[state], type: ['Human']*len(human_distance[state]) + ['Optimal']*len(optimal_distance[state])})\n",
    "    sns.histplot(data=data, x=x_axis, hue=type, discrete=discrete, multiple='layer', shrink=0.8)\n",
    "    plt.text(0.5, 1, f\"Cohen's distance: {cohen_d(human_distance[state], optimal_distance[state]):.{3}g}, p-value: {ttest_rel(human_distance[state], optimal_distance[state]).pvalue:.{3}g}\", horizontalalignment='center', verticalalignment='bottom', transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "    plt.savefig(f'{title}_histogram_{prettify_state(state)}.pdf', format='pdf')\n",
    "    print(f'Cohens d {cohen_d(human_distance[state], optimal_distance[state])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in human_similarity.keys():\n",
    "    graph_histograms(state, human_similarity, optimal_similarity, file_path, \"Distance\", \"Type\", discrete=True, mult = 'dodge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_histograms(221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# I want to generate histograms of optimal_similary[14] and human_similarity[14] in one coll\n",
    "# sns.histplot(human_similarity[14], bins=30, alpha=0.5, label='Human')\n",
    "# sns.histplot(optimal_similarity[14], bins=30, alpha=0.5, label='Optimal')\n",
    "\n",
    "# I want a histogram that is normalized to 0 to 1\n",
    "# subsample 5000 random indices from the human_similarity[14] and optimal_similarity[14]\n",
    "idx = np.random.choice(len(human_similarity[14]), len(human_similarity[14]), replace=False)\n",
    "human_similarity_subsample = [human_similarity[14][i] for i in idx]\n",
    "optimal_similarity_subsample = [optimal_similarity[14][i] for i in idx]\n",
    "# sns.histplot(human_similarity_subsample, bins=(), alpha=0.5, label='Human', stat = 'count')\n",
    "# sns.histplot(optimal_similarity_subsample, bins=30, alpha=0.5, label='Optimal', stat = 'count', color = None)\n",
    "\n",
    "# the range is from -0.3 to 0.65, want 30 evenly spaced bins for both histograms\n",
    "sns.histplot(human_similarity[14], binrange=[-0.3, 0.65], bins=30, alpha=0.5, label='Human', stat = 'count')\n",
    "sns.histplot(optimal_similarity[14], binrange=[-0.3, 0.65], bins=30, alpha=0.5, label='Optimal', stat = 'count', color = None)\n",
    "\n",
    "# only include first and third label in legend\n",
    "plt.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
